{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "pv.start_xvfb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_info = pd.read_json(\"/home/aleksandr/mount-folder/3D-FRONT-texture/texture_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_room = pv.read_texture(\"/home/aleksandr/mount-folder/3D-FRONT-texture/273a7787-72f5-4642-b333-03d06526035b/texture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/aleksandr/mount-folder/3D-FUTURE-model-part1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"existing_models.txt\")\n",
    "existing_model_ids = set(f.read().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>super-category</th>\n",
       "      <th>category</th>\n",
       "      <th>style</th>\n",
       "      <th>theme</th>\n",
       "      <th>material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f89da2db-ad8c-4582-b186-ed2a46f3cb15</td>\n",
       "      <td>Sofa</td>\n",
       "      <td>armchair</td>\n",
       "      <td>Minimalist</td>\n",
       "      <td>Gold Foil</td>\n",
       "      <td>Rough Cloth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5de45849-7d1b-4378-82da-ed183b7ecc37</td>\n",
       "      <td>Chair</td>\n",
       "      <td>Lounge Chair / Cafe Chair / Office Chair</td>\n",
       "      <td>Ming Qing</td>\n",
       "      <td>Floral</td>\n",
       "      <td>Rough Cloth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ef668a9-12e0-447b-9bbc-8ae484ba8c58</td>\n",
       "      <td>Lighting</td>\n",
       "      <td>Pendant Lamp</td>\n",
       "      <td>Modern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0d3e3b3c-3f1a-47ee-8566-1052cb8635b6</td>\n",
       "      <td>Cabinet/Shelf/Desk</td>\n",
       "      <td>Coffee Table</td>\n",
       "      <td>Modern</td>\n",
       "      <td>Texture Mark</td>\n",
       "      <td>Wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16febe29-52d9-4833-a695-b09446e65512</td>\n",
       "      <td>Cabinet/Shelf/Desk</td>\n",
       "      <td>Corner/Side Table</td>\n",
       "      <td>Modern</td>\n",
       "      <td>Cartoon</td>\n",
       "      <td>Composite Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16558</th>\n",
       "      <td>328918d2-1121-4f2e-bb63-fb1ecadc687a</td>\n",
       "      <td>Others</td>\n",
       "      <td>None</td>\n",
       "      <td>Modern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16559</th>\n",
       "      <td>5e5c01c1-e9a0-4eff-a059-0c9ea5afae9c</td>\n",
       "      <td>Lighting</td>\n",
       "      <td>Ceiling Lamp</td>\n",
       "      <td>Modern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16560</th>\n",
       "      <td>908e38ac-35ca-4aa2-ad9f-8402fc7ae8ed</td>\n",
       "      <td>Others</td>\n",
       "      <td>None</td>\n",
       "      <td>Modern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16561</th>\n",
       "      <td>966e026d-c284-438e-99a1-2baba6379d3f</td>\n",
       "      <td>Others</td>\n",
       "      <td>None</td>\n",
       "      <td>Modern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16562</th>\n",
       "      <td>c0552bff-62ad-451e-b726-2faade1a02ab</td>\n",
       "      <td>Others</td>\n",
       "      <td>None</td>\n",
       "      <td>Modern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16563 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model_id      super-category  \\\n",
       "0      f89da2db-ad8c-4582-b186-ed2a46f3cb15                Sofa   \n",
       "1      5de45849-7d1b-4378-82da-ed183b7ecc37               Chair   \n",
       "2      1ef668a9-12e0-447b-9bbc-8ae484ba8c58            Lighting   \n",
       "3      0d3e3b3c-3f1a-47ee-8566-1052cb8635b6  Cabinet/Shelf/Desk   \n",
       "4      16febe29-52d9-4833-a695-b09446e65512  Cabinet/Shelf/Desk   \n",
       "...                                     ...                 ...   \n",
       "16558  328918d2-1121-4f2e-bb63-fb1ecadc687a              Others   \n",
       "16559  5e5c01c1-e9a0-4eff-a059-0c9ea5afae9c            Lighting   \n",
       "16560  908e38ac-35ca-4aa2-ad9f-8402fc7ae8ed              Others   \n",
       "16561  966e026d-c284-438e-99a1-2baba6379d3f              Others   \n",
       "16562  c0552bff-62ad-451e-b726-2faade1a02ab              Others   \n",
       "\n",
       "                                       category       style         theme  \\\n",
       "0                                      armchair  Minimalist     Gold Foil   \n",
       "1      Lounge Chair / Cafe Chair / Office Chair   Ming Qing        Floral   \n",
       "2                                  Pendant Lamp      Modern          None   \n",
       "3                                  Coffee Table      Modern  Texture Mark   \n",
       "4                             Corner/Side Table      Modern       Cartoon   \n",
       "...                                         ...         ...           ...   \n",
       "16558                                      None      Modern          None   \n",
       "16559                              Ceiling Lamp      Modern          None   \n",
       "16560                                      None      Modern          None   \n",
       "16561                                      None      Modern          None   \n",
       "16562                                      None      Modern          None   \n",
       "\n",
       "              material  \n",
       "0          Rough Cloth  \n",
       "1          Rough Cloth  \n",
       "2                 None  \n",
       "3                 Wood  \n",
       "4      Composite Board  \n",
       "...                ...  \n",
       "16558             None  \n",
       "16559             None  \n",
       "16560             None  \n",
       "16561             None  \n",
       "16562             None  \n",
       "\n",
       "[16563 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = pd.read_json(\"/home/aleksandr/mount-folder/3D-FUTURE-model-part1/model_info.json\")\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_info.groupby(\"super-category\")[\"category\"].unique().explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "dir = \"/home/aleksandr/mount-folder/3D-FUTURE-model-part1\"\n",
    "# model_ids = existing_model_ids & set(model_info[\"model_id\"])\n",
    "model_ids = existing_model_ids & set(model_info[model_info[\"category\"] == \"King-size Bed\"][\"model_id\"])\n",
    "model_ids = list(model_ids)\n",
    "models = []\n",
    "for name in model_ids[:n]:\n",
    "    f = os.path.join(dir, name)\n",
    "    models.append({\n",
    "        \"model\": pv.read(os.path.join(f, \"normalized_model.obj\")),\n",
    "        \"texture\": pv.read_texture(os.path.join(f, \"texture.png\"))\n",
    "    })\n",
    "# m = pv.read(\"/home/aleksandr/mount-folder/3D-FUTURE-model-part1/ffde330f-dce0-4215-bfe6-203fe615b609/raw_model.obj\")\n",
    "# norm_m = pv.read(\"/home/aleksandr/mount-folder/3D-FUTURE-model-part1/ffde330f-dce0-4215-bfe6-203fe615b609/normalized_model.obj\")\n",
    "# tex = pv.read_texture(\"/home/aleksandr/mount-folder/3D-FUTURE-model-part1/ffde330f-dce0-4215-bfe6-203fe615b609/texture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_object(image_array):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    gray = np.where(gray==255, 0, gray)\n",
    "\n",
    "    # # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # # Find the largest contour (assumed to be the object)\n",
    "    largest_contour = max(contours, key=cv2.contourArea).squeeze()\n",
    "\n",
    "    bottom_right = largest_contour.max(0)\n",
    "    top_left = largest_contour.min(0)\n",
    "\n",
    "    # TODO: add boundaries checker\n",
    "    cropped_image_array = image_array[top_left[1]: bottom_right[1] + 1,\n",
    "                                      top_left[0]: bottom_right[0] + 1, :]\n",
    "    return cropped_image_array\n",
    "\n",
    "def create_views(mesh, texture, camera_distance, positions):\n",
    "    model_views = []\n",
    "    for position in positions:\n",
    "        pl = pv.Plotter()\n",
    "        pl.camera_position = position\n",
    "        no_transp_texture = pv.numpy_to_texture(texture.to_array()[..., :3])\n",
    "        pl.add_mesh(mesh, texture=no_transp_texture)\n",
    "        model_views.append(crop_object(pl.screenshot(return_img=True)))\n",
    "    return model_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = create_views(models[0][\"model\"], models[0][\"texture\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # axes = pv.Axes(show_actor=True, actor_scale=1.0)\n",
    "# camera_distance=5\n",
    "\n",
    "# # front_view_camera_position = np.array([(0, -camera_distance, 0), (0, 0, 0), (0, 0, 1)])\n",
    "# # rotated_camera_position = front_view_camera_position.dot(np.array([[-1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n",
    "# # plotter.camera_position = rotated_camera_position\n",
    "# positions = [\n",
    "#     [(0, camera_distance, 0), (0, 0, 0), (0, 0, -1)],\n",
    "#     [(0, -camera_distance, 0), (0, 0, 0), (0, 0, -1)],\n",
    "#     [(0, 0, camera_distance), (0, 0, 0), (0, 1, 0)],\n",
    "#     [(-camera_distance, 0, 0), (0, 0, 0), (0, 1, 0)],\n",
    "#     [(camera_distance, 0, 0), (0, 0, 0), (0, 1, 0)],\n",
    "#     [(0, 0, -camera_distance), (0, 0, 0), (0, 1, 0)],\n",
    "# ]\n",
    "# views = []\n",
    "# for m in models:\n",
    "# #     pl.camera_position = [(3.2257270137110754, 3.640847078923325, 3.2256104866305395),\n",
    "# #  (0.0, 0.41512006521224976, -0.00011652708053588867),\n",
    "# #  (0.0, 0.0, 1.0)]\n",
    "#     # pl.camera.zoom(0.3)\n",
    "#     # pl.add_actor(axes.actor)\n",
    "#     one_model_views = []\n",
    "#     for position in positions:\n",
    "#         pl = pv.Plotter()\n",
    "#         pl.camera_position = position\n",
    "#         pl.add_mesh(m[\"model\"], texture=m[\"texture\"])\n",
    "#         # pl.add_texture(texture=m[\"texture\"])\n",
    "#         one_model_views.append(crop_object(pl.screenshot(return_img=True)))\n",
    "#     views.append(one_model_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', feature_layer=None):\n",
    "        self.model_name = model_name\n",
    "        self.feature_layer = feature_layer\n",
    "        \n",
    "        # Load the pretrained model\n",
    "        self.model = self._load_pretrained_model()\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Define the transformations\n",
    "        self.transform = self._get_transform()\n",
    "    \n",
    "    def _load_pretrained_model(self):\n",
    "        # Load the pretrained model from torchvision\n",
    "        if self.model_name == 'resnet50':\n",
    "            model = models.resnet50(pretrained=True)\n",
    "        elif self.model_name == 'vgg16':\n",
    "            model = models.vgg16(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid model_name. Supported models: resnet50, vgg16')\n",
    "        \n",
    "        # Replace the last fully connected layer with an identity layer\n",
    "        if self.feature_layer is not None:\n",
    "            model.fc = torch.nn.Identity()\n",
    "        \n",
    "        # Move the model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _get_transform(self):\n",
    "        # Define the transformations based on the chosen model\n",
    "        if self.model_name == 'resnet50':\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        elif self.model_name == 'vgg16':\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError('Invalid model_name. Supported models: resnet50, vgg16')\n",
    "        \n",
    "        return transform\n",
    "    \n",
    "    def extract_features(self, images):\n",
    "        _images = images if isinstance(images, list) else [images]\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        transformed_images = []\n",
    "        for image in _images:\n",
    "            transformed_images.append(self.transform(image))\n",
    "        transformed_images = torch.stack(transformed_images)\n",
    "        \n",
    "        # Move the image to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            transformed_images = transformed_images.cuda()\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            features = self.model(transformed_images)\n",
    "        \n",
    "        # Return the concatenated features\n",
    "        return torch.cat(features.cpu().squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleRanker:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name=\"resnet50\",\n",
    "        feature_layer=None, \n",
    "        positions=None, \n",
    "        camera_distance=5\n",
    "    ):\n",
    "        self.feature_extractor = FeatureExtractor(model_name=model_name, feature_layer=feature_layer)\n",
    "        self.cossim = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        self.camera_distance = camera_distance\n",
    "        if not positions:\n",
    "            self.positions = [\n",
    "                [(0, camera_distance, 0), (0, 0, 0), (0, 0, -1)],\n",
    "                [(0, -camera_distance, 0), (0, 0, 0), (0, 0, -1)],\n",
    "                [(0, 0, camera_distance), (0, 0, 0), (0, 1, 0)],\n",
    "                [(-camera_distance, 0, 0), (0, 0, 0), (0, 1, 0)],\n",
    "                [(camera_distance, 0, 0), (0, 0, 0), (0, 1, 0)],\n",
    "                [(0, 0, -camera_distance), (0, 0, 0), (0, 1, 0)],\n",
    "            ]\n",
    "    \n",
    "    def _extract_features_from_object(self, object):\n",
    "            object_views = create_views(\n",
    "                object[\"model\"], \n",
    "                object[\"texture\"], \n",
    "                camera_distance=self.camera_distance,\n",
    "                positions=self.positions)\n",
    "            return self.feature_extractor.extract_features(object_views)\n",
    "\n",
    "    def __call__(self, context, targets):\n",
    "        _targets = targets if isinstance(targets, list) else [targets]\n",
    "        _context = context if isinstance(context, list) else [context]\n",
    "\n",
    "        aggregated_features = None\n",
    "        for context_object in _context:\n",
    "            features = self._extract_features_from_object(context_object)\n",
    "            aggregated_features = (\n",
    "                aggregated_features + features) if aggregated_features else features\n",
    "\n",
    "        scores = torch.zeros(len(_targets))\n",
    "        for idx, target in enumerate(_targets):\n",
    "            features = self._extract_features_from_object(target)\n",
    "            scores[i] = self.cossim(aggregated_features, features)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "space_match",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
